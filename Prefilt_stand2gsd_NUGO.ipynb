{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2983d20f-db18-478c-9102-316c56b1d778",
   "metadata": {},
   "source": [
    "# 0. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a2bbc-7ae9-4bdf-a9de-4b9954cb1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openpyxl\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# 나눔 고딕 적용\n",
    "matplotlib.rc(\"font\", family = \"NanumGothic\")\n",
    "# 음수 표시\n",
    "matplotlib.rc(\"axes\", unicode_minus = False)\n",
    "\n",
    "# 데이터 분할:train, test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205ecfb-9741-43f9-9a0d-9ff1d0b9bb58",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da54be4-7ba6-421e-993f-b6d5e8a6aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'file_path' : '/home/piai/AIweek/AI_PROJECT/Develop/DialectNLP/',\n",
    "       'data_path' : '/home/piai/AIweek/AI_PROJECT/Develop/DialectNLP/dataset/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf3482-da4d-4716-8d02-c04a65b47412",
   "metadata": {},
   "source": [
    "# 2. Preprocessing!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70608a4a-dba7-4fdd-b3db-10887a93235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 텍스트 파일 열기\n",
    "# # DKSR20000890 ~ DKSR20001736\n",
    "# for _ in range\n",
    "#     with open(cfg['file_path']+'(new3)라벨링데이터/DKSR20000890.txt', 'r', encoding='utf-8') as file:\n",
    "#         # 파일의 모든 내용을 읽고 출력하기\n",
    "#         content = file.read()\n",
    "#         # print(content)\n",
    "\n",
    "stand_list = list()\n",
    "gsd_list = list()\n",
    "# DKSR20000890부터 DKSR20001736까지의 파일을 순회\n",
    "for i in range(890, 1737):\n",
    "    # 파일 이름 생성\n",
    "    file_name = f'DKSR2000{i:04}.txt'\n",
    "    \n",
    "    # 파일 전체 경로\n",
    "    full_path = os.path.join(cfg['file_path'], '(new3)라벨링데이터/', file_name)\n",
    "\n",
    "    # 파일 열기 및 내용 읽기\n",
    "    try:\n",
    "        with open(full_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            # 파일 내용 처리 (예: 출력)\n",
    "            # print(content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일이 존재하지 않습니다: {file_name}\")\n",
    "\n",
    "    # (())라는 문자열을 제거\n",
    "    p = re.compile('\\(\\(\\)\\) ')\n",
    "    content_2 = p.sub('', content)\n",
    "    # print(content_2)\n",
    "    \n",
    "    # 줄바꿈(\\n)을 띄어쓰기(' ')로 바꾸기\n",
    "    p = re.compile('\\\\n')\n",
    "    content_3 = p.sub(' ', content_2)\n",
    "    # print(content_3)\n",
    "    \n",
    "    def replace_with_newline(match, replacements=[0]):\n",
    "        if replacements[0] == 0:\n",
    "            # 첫 번째 매치는 바꾸지 않음\n",
    "            replacements[0] += 1\n",
    "            return match.group()[2:]\n",
    "        else:\n",
    "            # 두 번째부터 '1:'를 지우고 줄바꿈으로 대체\n",
    "            return '\\n' + match.group()[2:]  # '1:' 부분을 제외하고 나머지를 반환\n",
    "    \n",
    "    p = re.compile('1: ')\n",
    "    # 커스텀 교체 함수 사용\n",
    "    content_4 = p.sub(replace_with_newline, content_3)\n",
    "    # print(content_4)\n",
    "    \n",
    "    p = re.compile('2: ')\n",
    "    # 커스텀 교체 함수 사용\n",
    "    content_5 = p.sub(replace_with_newline, content_4)\n",
    "    # print(content_5)\n",
    "    \n",
    "    # '--' 사이의 한글 단어를 찾아 제거하는 정규 표현식\n",
    "    pattern = r'-[가-힣]+- '\n",
    "    # 정규 표현식을 사용하여 문장에서 패턴에 해당하는 부분 제거\n",
    "    content_6 = re.sub(pattern, '', content_5)\n",
    "    # print(content_6)\n",
    "    \n",
    "    # 한 글자 + '~' 패턴을 찾아 제거하는 정규 표현식\n",
    "    pattern = r'[가-힣]~ '\n",
    "    # 정규 표현식을 사용하여 패턴에 해당하는 부분 제거\n",
    "    content_7 = re.sub(pattern, '', content_6)\n",
    "    # print(content_7)\n",
    "    \n",
    "    # 한 글자 + '~' 패턴을 찾아 제거하는 정규 표현식\n",
    "    pattern = r'@\\S+ '\n",
    "    # 정규 표현식을 사용하여 패턴에 해당하는 부분 제거\n",
    "    content_8 = re.sub(pattern, '', content_7)\n",
    "    # print(content_8)\n",
    "    \n",
    "    # 한 글자 + '~' 패턴을 찾아 제거하는 정규 표현식\n",
    "    pattern = '\\n'\n",
    "    # 정규 표현식을 사용하여 패턴에 해당하는 부분 제거\n",
    "    content_9 = re.sub(pattern, '', content_8)\n",
    "    # print(content_9)\n",
    "    \n",
    "\n",
    "    # '(A)/(B)' 형식에서 A만 남기는 정규 표현식\n",
    "    pattern_A = r'\\(([^)]+)\\)/\\([^)]+\\)'\n",
    "    # '(A)/(B)' 형식에서 B만 남기는 정규 표현식\n",
    "    pattern_B = r'\\([^)]+\\)/\\(([^)]+)\\)'\n",
    "    \n",
    "    # 정규 표현식을 사용하여 첫 번째 괄호 안의 내용만 남김\n",
    "    stand = re.sub(pattern_B, r'\\1', content_9)\n",
    "    gsd = re.sub(pattern_A, r'\\1', content_9)\n",
    "    # print(stand)\n",
    "    \n",
    "    # 한 글자 + '~' 패턴을 찾아 제거하는 정규 표현식\n",
    "    pattern = '\\n'\n",
    "    # 정규 표현식을 사용하여 패턴에 해당하는 부분 제거\n",
    "    stand = re.sub(pattern, '', stand)\n",
    "    gsd = re.sub(pattern, '', gsd)\n",
    "\n",
    "    stand_list.append(stand)\n",
    "    gsd_list.append(gsd)\n",
    "\n",
    "    # 빈 DataFrame 생성\n",
    "    globals()[f'df_{i}'] = pd.DataFrame(columns=['stand', 'gsd'])\n",
    "    \n",
    "    # 문장 분리 및 구두점 유지\n",
    "    sentences = re.findall(r'[^.!?]+[.!?]', stand)\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "    # 1열에 문장 저장\n",
    "    globals()[f'df_{i}']['stand'] = pd.Series(sentences)\n",
    "    \n",
    "    # 문장 분리 및 구두점 유지\n",
    "    sentences = re.findall(r'[^.!?]+[.!?]', gsd)\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "    # 1열에 문장 저장\n",
    "    globals()[f'df_{i}']['gsd'] = pd.Series(sentences)\n",
    "\n",
    "# 연결할 데이터프레임 목록 생성 (제외할 데이터프레임은 건너뜀)\n",
    "dataframes_to_concat = [globals()[f'df_{i}'] for i in range(890, 1737) \n",
    "                        if i not in (1271, 1273, 1294, 1702)]\n",
    "\n",
    "# 데이터프레임 연결\n",
    "df_combined = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b77fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b0fdf-2cf9-4adf-a436-c7768dd7d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d760a-ebcb-431b-9c67-09f6e3e53daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0c54d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 데이터프레임을 넘파이 배열로 변환\n",
    "numpy_array = df_combined.to_numpy()\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 넘파이 행렬을 리스트로 변환\n",
    "# df2 = numpy_array.tolist()\n",
    "# df2\n",
    "df2 = numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6492d81-0c46-49b3-bbe3-e5e3cffdebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))\n",
    "# 중복된 리스트를 제거하여 유니크한 리스트만 남김\n",
    "df3 = [list(x) for x in set(tuple(x) for x in df2)]\n",
    "\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479197ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f33708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_float(lst):\n",
    "    for item in lst:\n",
    "        if isinstance(item, float):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 리스트의 값 중에서 float 값이 있는지 확인\n",
    "contains_float_value = contains_float(df3)\n",
    "\n",
    "if contains_float_value:\n",
    "    print(\"리스트의 값 중에 float 값이 포함되어 있습니다.\")\n",
    "else:\n",
    "    print(\"리스트의 값 중에 float 값이 포함되어 있지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 형식 변경\n",
    "df4 = [{'standard': pair[0], 'dialect': pair[1]} for pair in df3]\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf886d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 문자열이 아닌 부분을 찾아서 출력\n",
    "for d in df4:\n",
    "    if not isinstance(d['standard'], str) or not isinstance(d['dialect'], str):\n",
    "        print(\"문자열이 아닌 데이터 발견:\")\n",
    "        print(\"standard:\", d['standard'])\n",
    "        print(\"dialect:\", d['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e60c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열이 아닌 데이터를 제외한 데이터셋을 생성\n",
    "df5 = []\n",
    "print(len(df4))\n",
    "for d in df4:\n",
    "    if isinstance(d['standard'], str) and isinstance(d['dialect'], str):\n",
    "        df5.append(d)\n",
    "print(len(df5))\n",
    "type(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9249ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(df5)\n",
    "# nan_locations = temp.isna()\n",
    "# print(nan_locations)\n",
    "nan_count = temp.isna().sum().sum()\n",
    "print(f\"NaN 값의 개수: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 8:2 비율로 분할\n",
    "train, test = train_test_split(df5, test_size=0.3, random_state=42)\n",
    "\n",
    "# 분할된 데이터의 크기 확인\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc951882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open(cfg['data_path']+'sent_gs_train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train[:1000], f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "with open(cfg['data_path']+'sent_gs_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test[:100], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"딕셔너리가 'sent_gs_train.json' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readJson(self, path):\n",
    "#     with open(path, 'r', encoding='utf8') as f:\n",
    "#         data = json.load(f)\n",
    "#     # 문자열이 아닌 데이터 필터링\n",
    "#     data = [d for d in data if isinstance(d['standard'], str) and isinstance(d['dialect'], str)]\n",
    "#     self.pairs = [[self._normalize(d['standard']), self._normalize(d['dialect'])] for d in data]\n",
    "#     self._filterPairs()\n",
    "#     self.srcs = [pair[0] for pair in self.pairs]\n",
    "#     self.trgs = [pair[1] for pair in self.pairs]\n",
    "\n",
    "# def readJson(self, path):\n",
    "#     with open(path, 'r', encoding='utf8') as f:\n",
    "#         data = json.load(f)\n",
    "#     self.pairs = [[self._normalize(d['standard']), self._normalize(d['dialect'])] for d in data if isinstance(d['standard'], str) and isinstance(d['dialect'], str)]\n",
    "#     self._filterPairs()\n",
    "#     self.srcs = [pair[0] for pair in self.pairs]\n",
    "#     self.trgs = [pair[1] for pair in self.pairs]\n",
    "\n",
    "# def readJson(self, path):\n",
    "#     with open(path, 'r', encoding='utf8') as f:\n",
    "#         data = json.load(f)\n",
    "#     self.pairs = []\n",
    "#     for d in data:\n",
    "#         standard = d.get('standard', None)\n",
    "#         dialect = d.get('dialect', None)\n",
    "#         if standard is not None and dialect is not None and isinstance(standard, str) and isinstance(dialect, str):\n",
    "#             self.pairs.append([self._normalize(standard), self._normalize(dialect)])\n",
    "#     self._filterPairs()\n",
    "#     self.srcs = [pair[0] for pair in self.pairs]\n",
    "#     self.trgs = [pair[1] for pair in self.pairs]\n",
    "\n",
    "# 데이터프레임을 사용하여 JSON 파일 읽기\n",
    "def readJson(self, path):\n",
    "    df = pd.read_json(path, encoding='utf8')\n",
    "    df = df.fillna('')  # NaN 값을 빈 문자열로 대체\n",
    "    self.pairs = [[self._normalize(d['standard']), self._normalize(d['dialect'])] for _, d in df.iterrows()]  # 수정\n",
    "    self._filterPairs()\n",
    "    self.srcs = [pair[0] for pair in self.pairs]\n",
    "    self.trgs = [pair[1] for pair in self.pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264f5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JSON 파일 경로\n",
    "train_path = cfg['data_path']+'sent_gs_train.json'\n",
    "test_path = cfg['data_path']+'sent_gs_test.json'\n",
    "\n",
    "# Loader 인스턴스 생성\n",
    "loader = Loader(MAX_LENGTH, INPUT_LEVEL)\n",
    "\n",
    "# JSON 파일 읽기\n",
    "loader.readJson(train_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(loader.pairs)  # [ [standard_text, dialect_text], ... ]\n",
    "print(loader.srcs)   # [ standard_text, ... ]\n",
    "print(loader.trgs)   # [ dialect_text, ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 형식 확인\n",
    "for d in df4[:10]:\n",
    "    print(type(d['standard']), type(d['dialect']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df697294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 간단한 Encoder 클래스 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)                \n",
    "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "# 간단한 Attention 클래스 정의\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))         \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "        \n",
    "        #energy = energy.squeeze(1)  # 새로운 차원 제거\n",
    "        energy = energy.permute(0, 2, 1)        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "                \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "# 간단한 Decoder 클래스 정의\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        #weighted = weighted.squeeze(1)  # 새로운 차원 제거\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)\n",
    "\n",
    "# Seq2Seq 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, sos_idx, device, max_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sos_idx = sos_idx\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        if trg is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "            trg = torch.zeros((self.max_length, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n",
    "        else:\n",
    "            inference = False\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class Vocab:\n",
    "    def __init__(self, sentences, input_level, device):\n",
    "        self.sentences = sentences\n",
    "        self.stoi = {'<pad>': 0, '<sos>': 1, '<eos>':2, '<unk>':3}\n",
    "        self.s_freq = {'<pad>': 0, '<sos>': 0, '<eos>':0, '<unk>':0}\n",
    "        self.itos = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "        self.vocab_size = 4\n",
    "        self.PAD_IDX = 0\n",
    "        self.SOS_IDX = 1\n",
    "        self.EOS_IDX = 2\n",
    "        self.UNK_IDX = 3\n",
    "        self.input_level = input_level\n",
    "        self.device = device\n",
    "    \n",
    "    def _addWord(self, s):\n",
    "        if s not in self.stoi:\n",
    "            self.stoi[s] = self.vocab_size\n",
    "            self.s_freq[s] = 1\n",
    "            self.itos[self.vocab_size] = s\n",
    "            self.vocab_size += 1\n",
    "        else:\n",
    "            self.s_freq[s] += 1\n",
    "\n",
    "    def build_vocab(self):\n",
    "        for sentence in self.sentences:\n",
    "            if self.input_level == 'syl':\n",
    "                sentence = [ch for ch in sentence]\n",
    "            elif self.input_level == 'word':\n",
    "                sentence = sentence.split()\n",
    "            elif self.input_level == 'jaso':\n",
    "                print(\"NOT IMPLEMENTED!\")\n",
    "                exit()\n",
    "\n",
    "            for s in sentence:\n",
    "                self._addWord(s)\n",
    "    \n",
    "    def sentenceFromIndex(self, indexs):\n",
    "        ret_list = []\n",
    "        for t in indexs:\n",
    "            if t in self.itos:\n",
    "                ret_list.append(self.itos[t])\n",
    "            else:   \n",
    "                ret_list.append('<unk>')\n",
    "        return ret_list\n",
    "\n",
    "    def indexesFromSentence(self, sentence, sos, eos):\n",
    "        ret_list = []\n",
    "        if sos:\n",
    "            ret_list.append(self.SOS_IDX)\n",
    "\n",
    "        if self.input_level == 'syl':\n",
    "            sentence = [ch for ch in sentence]\n",
    "        elif self.input_level == 'word':\n",
    "            sentence = sentence.split()\n",
    "        elif self.input_level == 'jaso':\n",
    "            print(\"NOT IMPLEMENTED!\")\n",
    "            exit()\n",
    "\n",
    "        for s in sentence:\n",
    "            if s in self.stoi:\n",
    "                ret_list.append(self.stoi[s])\n",
    "            else:\n",
    "                ret_list.append(self.UNK_IDX)\n",
    "        if eos:\n",
    "            ret_list.append(self.EOS_IDX)\n",
    "        return ret_list\n",
    "\n",
    "    def tensorFromSentence(self, sentence, sos, eos):\n",
    "        indexes = self.indexesFromSentence(sentence, sos, eos)\n",
    "        return torch.tensor(indexes, dtype=torch.long, device=self.device).view(-1, 1)\n",
    "        # return = [sent len, batch size]\n",
    "        # return = [sent len, 1]\n",
    "        \n",
    "class Loader:\n",
    "    def __init__(self, max_len, input_level):\n",
    "        self.pairs = []\n",
    "        self.srcs = []\n",
    "        self.trgs = []\n",
    "        self.max_len = max_len\n",
    "        self.input_level = input_level\n",
    "\n",
    "    def _normalize(self, sentence):\n",
    "        sentence = sentence.strip()\n",
    "        sentence = re.sub(r\"[^가-힣 ]\", r\"\", sentence)\n",
    "        return sentence\n",
    "    \n",
    "    def _filterPairs(self):\n",
    "        filtered = []\n",
    "        for pair in self.pairs:\n",
    "            if self.input_level == 'syl':\n",
    "                len_p0 = len(pair[0])\n",
    "                len_p1 = len(pair[1])\n",
    "            if self.input_level == 'word':\n",
    "                len_p0 = len(pair[0].split())\n",
    "                len_p1 = len(pair[1].split())\n",
    "\n",
    "            if len_p0 < self.max_len and len_p1 < self.max_len:\n",
    "                filtered.append(pair)\n",
    "        self.pairs = filtered\n",
    "\n",
    "    def readJson(self, path):\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            data = json.load(f)\n",
    "        self.pairs = [[self._normalize(d['standard']), self._normalize(d['dialect'])] for d in data]\n",
    "        self._filterPairs()\n",
    "        self.srcs = [pair[0] for pair in self.pairs]\n",
    "        self.trgs = [pair[1] for pair in self.pairs]\n",
    "        \n",
    "    def makeIterator(self, SRC, TRG, batch_size):\n",
    "        src_tensors = []\n",
    "        trg_tensors = []\n",
    "        sos = 1  # 시작 토큰\n",
    "        eos = 2  # 종료 토큰\n",
    "\n",
    "        for pair in self.pairs:\n",
    "            src = SRC.tensorFromSentence(pair[0], sos, eos)\n",
    "            trg = TRG.tensorFromSentence(pair[1], sos, eos)\n",
    "            src_tensors.append(src)\n",
    "            trg_tensors.append(trg)\n",
    "\n",
    "        dataset = TensorDataset(src_tensors, trg_tensors)\n",
    "        iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return iterator\n",
    "        \n",
    "#     def makeIterator(self, SRC, TRG, batch_size):\n",
    "#         src_tensors = []\n",
    "#         trg_tensors = []\n",
    "#         for pair in self.pairs:\n",
    "#             src = SRC.tensorFromSentence(pair[0], sos, eos)\n",
    "#             trg = TRG.tensorFromSentence(pair[1], sos, eos)\n",
    "#             src_tensors.append(src)\n",
    "#             trg_tensors.append(trg)\n",
    "\n",
    "#         # 패딩을 적용하여 텐서 길이를 맞춰줍니다.\n",
    "#         src_tensors = pad_sequence(src_tensors, batch_first=True)\n",
    "#         trg_tensors = pad_sequence(trg_tensors, batch_first=True)\n",
    "\n",
    "#         dataset = TensorDataset(src_tensors, trg_tensors)\n",
    "#         iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         return iterator\n",
    "        \n",
    "#     def makeIterator(self, SRC, TRG, batch_size):\n",
    "#         src_tensors = []\n",
    "#         trg_tensors = []\n",
    "#         sos = True  # 기본값 설정\n",
    "#         eos = True  # 기본값 설정\n",
    "#         for pair in self.pairs:\n",
    "#             src = SRC.tensorFromSentence(pair[0], sos, eos)\n",
    "#             trg = TRG.tensorFromSentence(pair[1], sos, eos)\n",
    "#             src_tensors.append(src)\n",
    "#             trg_tensors.append(trg)\n",
    "\n",
    "#         dataset = TensorDataset(src_tensors, trg_tensors)\n",
    "#         iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         return iterator\n",
    "        \n",
    "#     def makeIterator(self, batch_size, sos=True, eos=True, batch_size):\n",
    "\n",
    "#         # 입력 문장과 출력 문장을 각각 정리합니다.\n",
    "#         src_tensors = [self.src_vocab.tensorFromSentence(pair[0], sos, eos) for pair in self.pairs]\n",
    "#         trg_tensors = [self.trg_vocab.tensorFromSentence(pair[1], sos, eos) for pair in self.pairs]\n",
    "\n",
    "#         # 패딩을 추가합니다.\n",
    "#         padded_src_tensors = pad_sequence(src_tensors, padding_value=self.src_vocab.PAD_TOKEN, batch_first=True)\n",
    "#         padded_trg_tensors = pad_sequence(trg_tensors, padding_value=self.trg_vocab.PAD_TOKEN, batch_first=True)\n",
    "\n",
    "#         # 배치를 생성합니다.\n",
    "#         dataset = TensorDataset(padded_src_tensors, padded_trg_tensors)\n",
    "#         iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         return iterator\n",
    "        \n",
    "#     def makeIterator(self, SRC, TRG, sos, eos, batch_size):\n",
    "#         src_tensors = [SRC.tensorFromSentence(pair[0], sos, eos) for pair in self.pairs]\n",
    "#         trg_tensors = [TRG.tensorFromSentence(pair[1], sos, eos) for pair in self.pairs]\n",
    "\n",
    "#         # 입력 데이터의 차원을 조정합니다.\n",
    "#         src_tensors = torch.stack(src_tensors)\n",
    "#         trg_tensors = torch.stack(trg_tensors)\n",
    "\n",
    "#         dataset = TensorDataset(src_tensors, trg_tensors)\n",
    "#         iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#         return iterator\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "MAX_LENGTH = 150\n",
    "INPUT_LEVEL = 'syl'\n",
    "PATH_TRAIN = cfg['data_path']+'sent_gs_train.json'\n",
    "PATH_TEST = cfg['data_path']+'sent_gs_test.json'\n",
    "\n",
    "train_loader = Loader(MAX_LENGTH, INPUT_LEVEL)\n",
    "test_loader = Loader(MAX_LENGTH, INPUT_LEVEL)\n",
    "train_loader.readJson(PATH_TRAIN)\n",
    "test_loader.readJson(PATH_TEST)\n",
    "    \n",
    "SRC = Vocab(train_loader.srcs, INPUT_LEVEL, device)\n",
    "TRG = Vocab(train_loader.trgs, INPUT_LEVEL, device)\n",
    "SRC.build_vocab()\n",
    "TRG.build_vocab()\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "INPUT_DIM = 10000  # 입력 어휘 사이즈\n",
    "OUTPUT_DIM = 10000  # 출력 어휘 사이즈\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 12\n",
    "CLIP = 1\n",
    "\n",
    "PAD_IDX = TRG.stoi['<pad>']\n",
    "SOS_IDX = TRG.stoi['<sos>']\n",
    "EOS_IDX = TRG.stoi['<eos>']\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoder, Attention, Decoder 인스턴스 생성\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "# Seq2Seq 모델 인스턴스 생성\n",
    "model = Seq2Seq(enc, dec, SOS_IDX, device, MAX_LENGTH).to(device) \n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), cfg['data_path']+'s2sAttn_syl_gs_110_128_128')\n",
    "# # 모델을 저장할 때는 state_dict만 저장합니다.\n",
    "# torch.save(model.state_dict(), cfg['data_path']+'s2sAttn_syl_gs_110_128_128.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6379bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(cfg['data_path']+'s2sAttn_syl_gs_110_128_128.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a14c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time  # time 모듈을 임포트\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        if i % 10 == 0 or i == len(iterator) - 1:\n",
    "            print(f'\\r{i+1:4}/{len(iterator):4} {(i+1) * 100.0 /len(iterator):.1f}%', end='')\n",
    "\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "        \n",
    "        print('')\n",
    "        print(src)\n",
    "        \n",
    "        #src = src.squeeze(dim=2)  # 3차원으로 변환\n",
    "        #src = src.transpose(1, 0)  # 2차원으로 변환 (batch 크기 x src 문장 길이)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)        \n",
    "        loss.backward()        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print() \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0]\n",
    "            trg = batch[1]\n",
    "            \n",
    "            #src = src.squeeze(dim=2)  # 3차원으로 변환\n",
    "            #src = src.transpose(1, 0)  # 2차원으로 변환 (batch 크기 x src 문장 길이)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def train_model(model,\n",
    "                train_iterator, valid_iterator,\n",
    "                optimizer, criterion, CLIP, N_EPOCHS, model_pt_path):\n",
    "    best_valid_loss = float('inf')\n",
    "    last_exp_v = float('inf')\n",
    "    last_exp_t = float('inf')\n",
    "    inc_streak_v = 0\n",
    "    inc_streak_t = 0\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_pt_path)\n",
    "\n",
    "        exp_t = int(math.exp(train_loss))\n",
    "        exp_v = int(math.exp(valid_loss))\n",
    "        torch.save(model.state_dict(), model_pt_path.replace('.pt', f'_E{epoch}T{exp_t}V{exp_v}.pt'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        if exp_v > 20000:\n",
    "            break \n",
    "        if exp_v > last_exp_v:\n",
    "            inc_streak_v += 1\n",
    "        else :\n",
    "            inc_streak_v = 0\n",
    "        if exp_t > last_exp_t:\n",
    "            inc_streak_t += 1\n",
    "        else :\n",
    "            inc_streak_t = 0\n",
    "        \n",
    "        if inc_streak_v > 3 or inc_streak_t > 3:\n",
    "            break\n",
    "        last_exp_v = exp_v\n",
    "        last_exp_t = exp_t\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "# def collate_batch(batch):\n",
    "#     src_batch, trg_batch = zip(*batch)\n",
    "#     src_batch = pad_sequence(src_batch, batch_first=True)\n",
    "#     trg_batch = pad_sequence(trg_batch, batch_first=True)\n",
    "#     return src_batch, trg_batch\n",
    "        \n",
    "batch_size=32\n",
    "        \n",
    "# train_iterator = train_loader.makeIterator(SRC, TRG,\n",
    "#                                            sos=True, eos=True,\n",
    "#                                            batch_size=batch_size)\n",
    "# test_iterator = test_loader.makeIterator(SRC, TRG,\n",
    "#                                          sos=True, eos=True,\n",
    "#                                          batch_size=batch_size)\n",
    "# sos = 1\n",
    "# eos = 2\n",
    "\n",
    "train_iterator = train_loader.makeIterator(SRC, TRG, batch_size=batch_size)\n",
    "test_iterator = test_loader.makeIterator(SRC, TRG, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# portion = int(len(test_iterator) * 0.5)\n",
    "# valid_iterator = test_iterator[:portion]\n",
    "# test_iterator = test_iterator[portion:]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "N_EPOCHS = 12\n",
    "CLIP = 1\n",
    "\n",
    "# model_pt_path = f'./models/{model_name}/{model_name}.pt'\n",
    "model_pt_path = cfg['data_path'] + 's2sAttn_syl_gs_110_128_128.pt'\n",
    "        \n",
    "train_model(model = model, \n",
    "            train_iterator = train_iterator, \n",
    "            valid_iterator = valid_iterator, \n",
    "            optimizer = optimizer, \n",
    "            criterion = criterion, \n",
    "            CLIP = CLIP, \n",
    "            N_EPOCHS = N_EPOCHS, \n",
    "            model_pt_path = model_pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 8:2 비율로 분할\n",
    "train, test = train_test_split(df4, test_size=0.3, random_state=42)\n",
    "\n",
    "# 분할된 데이터의 크기 확인\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab08dd-3ed9-4588-bf07-cfbd9d316fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_stand.isnull().sum(axis = 0)\n",
    "# combined_stand.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4b692-c7a4-4d87-a69d-f687da86bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_combined))\n",
    "df_combined = df_combined.dropna(axis=0)\n",
    "print(len(df_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f9be6-a728-4e45-8d2c-77f6211e9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a966f25-0cbd-4116-8c08-899bb714f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 8:2 비율로 분할\n",
    "train, temp = train_test_split(df_combined, test_size=0.3, random_state=42)\n",
    "valid, test = train_test_split(temp, test_size=0.3, random_state=42)\n",
    "\n",
    "# 분할된 데이터의 크기 확인\n",
    "print(train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70331a68-1f1c-4da4-873e-9831f4c4c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일로 저장\n",
    "train.to_excel(cfg['file_path']+'dataset/train_stand2gsd.xlsx', index=False)\n",
    "valid.to_excel(cfg['file_path']+'dataset/valid_stand2gsd.xlsx', index=False)\n",
    "test.to_excel(cfg['file_path']+'dataset/test_stand2gsd.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92380f-c5e0-402e-922b-555ccde1681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 TSV 형식으로 저장\n",
    "train.to_csv(cfg['file_path']+'dataset/train.tsv', sep='\\t', index=False)\n",
    "valid.to_csv(cfg['file_path']+'dataset/test.tsv', sep='\\t', index=False)\n",
    "# test.to_csv(cfg['file_path']+'dataset/test_stand2gsd.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f247a7b-7447-46f8-b7ce-2396cdc0ff88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"stand\" : stand_list,\n",
    "#     \"gsd\" : gsd_list\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fe508-bdad-4084-85c9-186cda415f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # JSON 파일로 저장\n",
    "# with open(cfg['data_path']+'data.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(\"딕셔너리가 'data.json' 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8321555-5eb7-4042-bd20-47ae6fdfefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # 빈 DataFrame 생성\n",
    "    # globals()[f'df_{i}'] = pd.DataFrame(columns=['stand', 'gsd'])\n",
    "    \n",
    "    # # 문장 분리 및 구두점 유지\n",
    "    # sentences = re.findall(r'[^.!?]+[.!?]', stand)\n",
    "    # sentences = [sentence for sentence in sentences if sentence]\n",
    "    # # 1열에 문장 저장\n",
    "    # globals()[f'df_{i}']['stand'] = pd.Series(sentences)\n",
    "    \n",
    "    # # 문장 분리 및 구두점 유지\n",
    "    # sentences = re.findall(r'[^.!?]+[.!?]', gsd)\n",
    "    # sentences = [sentence for sentence in sentences if sentence]\n",
    "    # # 1열에 문장 저장\n",
    "    # globals()[f'df_{i}']['gsd'] = pd.Series(sentences)\n",
    "\n",
    "    # # 연결할 데이터프레임 목록 생성 (제외할 데이터프레임은 건너뜀)\n",
    "    # dataframes_to_concat = [globals()[f'df_{i}'] for i in range(890, 1737) \n",
    "    #                         if i not in (1271, 1273, 1294, 1702)]\n",
    "    \n",
    "    # # 데이터프레임 연결\n",
    "    # df_combined = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "    \n",
    "    # df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab9203-b272-4695-94c9-a5a61becd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 문장 분리 및 구두점 유지\n",
    "# list_stand = re.findall(r'[^.!?]+[.!?]', stand)\n",
    "# type(list_stand)\n",
    "\n",
    "# # 문장 분리 및 구두점 유지\n",
    "# list_gsd = re.findall(r'[^.!?]+[.!?]', gsd)\n",
    "# type(list_gsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfb5ed-5299-4ead-92de-2346e4e39987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     \"stand\" : list_stand,\n",
    "#     \"gsd\" : list_gsd,\n",
    "# }\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857503d-7451-4a99-9545-7544a8d6c6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb24e3-f229-4c10-b673-84e56c5370c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f79f9f-d6d8-4bd3-85e6-fa288871ac3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b645921-1b4b-4d67-a7ca-4b277631796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일로 저장\n",
    "df_combined.to_excel(cfg['file_path']+'dataset/sentences.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d082bc-1261-48cd-b7fa-c5b753780234",
   "metadata": {},
   "source": [
    "# 토큰화(Tokenization)\n",
    "링크 : https://data-minggeul.tistory.com/13?category=901822\n",
    "### (1) 토큰화 (Tokenization)\n",
    "    - 토큰화란 말뭉치(Corpus)를 토큰(Token)이라는 의미를 가지는 단위로 나누는 작업을 말합니다. 이때 토큰은 단어 혹은 문장 단위가 될 수 있습니다. 하지만 한국어에서는 의미 단위를 구분하기 위해 형태소를 고려해야 합니다.\n",
    "    - 형태소 단위의 토큰화를 위해서는 KoNLPy 패키지의 Okt, Mecab, Komoran, Hannanum, Kkma와 같은 형태소 분석기를 활용할 수 있습니다. 이 중 Mecab이 속도가 빠른 장점이 있어 주로 활용된다고 합니다.\n",
    "    - Mecab의 사용법은 아래와 같이 설치후 간단하게 사용할 수 있으며, 다른 형태소 분석기도 유사한 방식으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894b418-d06f-42ae-b395-0a7512812ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# https://huggingface.co/skt/kobert-base-v1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "morphs = tokenizer.encode(stand)\n",
    "len(morphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bda5c-0dc1-44fe-8696-044732da376a",
   "metadata": {},
   "source": [
    "### (2) 불용어(Stopword) 처리\n",
    "    - 문단/문장을 토큰화한 뒤 유의하지 않은 토큰들을 제거하는 작업이 필요합니다. 조사, 접미사와 같이 자주 등장하지만 의미가 없는 토큰들을 불용어라 부르고 학습 전 제거해줍니다.\n",
    "    - 불용어 처리를 위해 다른 사용자가 정의해놓은 불용어 사전을 활용하거나, 직접 정리한 불용어 사전을 사용하는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138a984-fb1d-4391-af5d-6e19d0caf6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 'punkt' 토크나이저 모델 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 텍스트 파일 열기\n",
    "with open(cfg['file_path']+'stopword.txt', 'r', encoding='utf-8') as file:\n",
    "    # 파일의 모든 내용을 읽고 출력하기\n",
    "    content = file.read()\n",
    "\n",
    "# 줄바꿈(\\n)을 띄어쓰기(' ')로 바꾸기\n",
    "p = re.compile('\\\\n')\n",
    "stopwords = p.sub(' ', content)\n",
    "stopwords = set(stopwords.split(' '))\n",
    "tok = word_tokenize(content_8)\n",
    "\n",
    "def stopword(word_tokenize):\n",
    "    result = []\n",
    "    for w in word_tokenize:\n",
    "        if w not in stopwords:\n",
    "            result.append(w)\n",
    "    return result\n",
    "\n",
    "result = stopword(tok)\n",
    "\n",
    "print('불용어 제거 전 :',tok) \n",
    "print()\n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e51d0-59e5-43f4-8932-fee012f02f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_proj",
   "language": "python",
   "name": "final_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
